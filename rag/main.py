import os
import asyncio
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_groq import ChatGroq
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

load_dotenv()

# [ì„¤ì •] ingest_db.pyì™€ ë™ì¼í•œ ì„¤ì • ìœ ì§€
PERSIST_DIRECTORY = "./chroma_advanced_db"
embeddings = OllamaEmbeddings(model="bge-m3")
reranker_model = HuggingFaceCrossEncoder(model_name="dragonkue/bge-reranker-v2-m3-ko")

# LLM ì„¤ì •
judge_llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
drafter_models = {
    "scout": ChatGroq(model="llama-3.1-8b-instant", temperature=0.7),
    "qwen": ChatGroq(model="qwen/qwen3-32b", temperature=0.7) # API ID í™•ì¸ í•„ìš”
}

def get_search_pipeline(vector_store):
    # 1. ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (BM25ìš©)
    all_docs = vector_store.get()["documents"]
    
    # 2. Base Retrievers
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": 15})
    bm25_retriever = BM25Retriever.from_texts(all_docs)
    bm25_retriever.k = 15
    
    # 3. Hybrid Ensemble (BM25 0.4 : Vector 0.6)
    ensemble = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.4, 0.6]
    )
    
    # 4. Reranking
    compressor = CrossEncoderReranker(model=reranker_model, top_n=5)
    final_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=ensemble
    )
    return final_retriever

async def generate_final_answer(query, context):
    """MoA êµ¬ì¡°ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (í•„í„°ë§ ê°•í™” ë²„ì „)"""
    
    # [ìˆ˜ì •] 1ì°¨ ë“œë˜í”„íŠ¸ ëª¨ë¸ë“¤ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ - ì—¬ê¸°ì„œë¶€í„° ì…ì„ ë§‰ì•„ì•¼ í•©ë‹ˆë‹¤.
    draft_prompt_template = """
    ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.
    
    [ê°€ì´ë“œë¼ì¸]
    1. [ì§ˆë¬¸]ì´ ì œê³µëœ [ë¬¸ë§¥]ê³¼ ì¡°ê¸ˆì´ë¼ë„ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    2. ë§Œì•½ ì§ˆë¬¸ì´ 'ë°°ê³ íŒŒ', 'ì¡¸ë ¤', 'ì¸ì‚¬' ë“± ì•± ë¶„ì„ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì´ë¼ë©´, êµ¬êµ¬ì ˆì ˆ ì„¤ëª…í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ë”± í•œ ë¬¸ì¥ "NO_RELATION"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
    3. ë§Œì•½ ì§ˆë¬¸ ëŒ€ìƒ ì•±ì´ 'ë„·í”Œë¦­ìŠ¤', 'ì™“ì± ', 'í‹°ë¹™', 'ë””ì¦ˆë‹ˆ+', 'ì¿ íŒ¡í”Œë ˆì´', 'Prime Video', 'ì• í”Œtv' ê°€ ì•„ë‹ ê²½ìš° "NOT_APP"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
    4. ê´€ë ¨ì´ ìˆë‹¤ë©´, ë¬¸ë§¥ì— ê¸°ë°˜í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ ì´ˆì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

    [ë¬¸ë§¥]: {context}
    [ì§ˆë¬¸]: {query}
    
    ë‹µë³€:
    """
    draft_prompt = ChatPromptTemplate.from_template(draft_prompt_template)
    
    # 1. ë“œë˜í”„íŠ¸ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰
    tasks = []
    model_labels = list(drafter_models.keys())
    for name, model in drafter_models.items():
        chain = draft_prompt | model | StrOutputParser()
        tasks.append(chain.ainvoke({"context": context, "query": query}))
    
    results = await asyncio.gather(*tasks)
    
    # [ì¶”ê°€] ëª¨ë“  ë“œë˜í”„íŠ¸ ëª¨ë¸ì´ ê±°ì ˆí–ˆëŠ”ì§€ í™•ì¸ (ì„±ëŠ¥ ìµœì í™”)
    if all("NO_RELATION" in res for res in results):
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ í•´ë‹¹ ì§ˆë¬¸ì— ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•±ì˜ ì„±ëŠ¥, ì‚¬ìš©ì ë°˜ì‘, ë²„ì „ ë¹„êµ ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"
    
    if all("NOT_APP" in res for res in results):
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” OTT ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ í•´ë‹¹ ì§ˆë¬¸ì— ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OTT ì•±ì— ê´€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"
    
    # 2. ê²°ê³¼ ì¢…í•©ìš© í”„ë¡¬í”„íŠ¸ (Judge LLM)
    candidates = ""
    for name, res in zip(model_labels, results):
        candidates += f"\n\n--- [ë¶„ì„ê°€: {name}] ---\n{res}"
    
    final_prompt = f"""
    ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì˜ê²¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    
    [ì°¸ì¡° ë°ì´í„°]
    {context}
    
    [ë¶„ì„ê°€ ì˜ê²¬ ëª¨ìŒ]
    {candidates}
    
    [ìµœì¢… ë‹µë³€ ê·œì¹™]
    - ë§Œì•½ ë¶„ì„ê°€ë“¤ì˜ ì˜ê²¬ì´ "NO_RELATION"ì´ê±°ë‚˜ ì§ˆë¬¸ì´ ì¼ìƒ ëŒ€í™”ë¼ë©´, ì–µì§€ë¡œ ì•±ê³¼ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”.
    - ë¬´ê´€í•œ ì§ˆë¬¸ì—ëŠ” "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì•± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•± ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
    - ê´€ë ¨ ìˆëŠ” ì§ˆë¬¸ì—ëŠ” ì¹œì ˆí•œ êµ¬ì–´ì²´(~í•´ìš”)ë¡œ í•µì‹¬ë§Œ ì§šì–´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {query}
    """
    
    final_answer = await judge_llm.ainvoke(final_prompt)
    return final_answer.content

async def main():
    # 1. ì €ì¥ëœ ë²¡í„° DB ë¡œë“œ
    print("ğŸ“‚ ë²¡í„° DB ë¡œë“œ ì¤‘...")
    vector_store = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings
    )
    
    # 2. ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    retriever = get_search_pipeline(vector_store)
    
    print("\nâœ… ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
    
    while True:
        query = input("\n[ì§ˆë¬¸]: ")
        if query.lower() in ['exit', 'quit', 'q', 'ì¢…ë£Œ']:
            break
            
        print("ğŸ” ê²€ìƒ‰ ë° ë¶„ì„ ì¤‘...")
        
        # A. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(query)
        
        # B. ë¬¸ë§¥ êµ¬ì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
        context_list = []
        for d in retrieved_docs:
            source_info = f"[ì•±: {d.metadata.get('app_name')}, ë²„ì „: {d.metadata.get('version')}, ë‚ ì§œ: {d.metadata.get('date')}]"
            context_list.append(f"{source_info}\në‚´ìš©: {d.page_content}")
        
        context = "\n\n".join(context_list)
        
        # C. ìµœì¢… ë‹µë³€ ìƒì„±
        answer = await generate_final_answer(query, context)
        
        print("\n" + "="*50)
        print("ğŸ¤– [AI ë‹µë³€]:")
        print(answer)
        print("="*50)

if __name__ == "__main__":
    asyncio.run(main())