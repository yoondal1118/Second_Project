import mysql.connector
import asyncio
import os
from datetime import datetime
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from keybert import KeyBERT
from konlpy.tag import Okt
from tqdm import tqdm
import json
import re

from dotenv import load_dotenv 
load_dotenv()

# [ì„¤ì •]
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
PERSIST_DIRECTORY = "./chroma_advanced_db"

embeddings = OllamaEmbeddings(model="bge-m3") 
reranker_model = HuggingFaceCrossEncoder(model_name="dragonkue/bge-reranker-v2-m3-ko")

judge_llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0, api_key=GROQ_API_KEY)
drafter_models = {
    "scout": ChatGroq(model="llama-3.1-8b-instant", temperature=0.7),
    "qwen": ChatGroq(model="qwen/qwen3-32b", temperature=0.7)
}

# DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('host'),
    'user': os.getenv('user'),
    'password': os.getenv('passwd'),
    'database': os.getenv('dbname')
}

class MetadataEnsemble:
    def __init__(self):
        self.kw_model = KeyBERT()
        self.okt = Okt()
    
    def _extract_local_keywords(self, text):
        nouns = " ".join(self.okt.nouns(text))
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬
        if not nouns.strip():
            return []
        try:
            keywords = self.kw_model.extract_keywords(nouns, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=5)
            return [k[0] for k in keywords]
        except:
            return []

    async def generate_metadata(self, text):
        # 1. ë¡œì»¬ ì¶”ì¶œ
        local_keywords = self._extract_local_keywords(text)
        
        # 2. LLM ì¶”ì¶œ (Groq)
        llm_prompt = f"""
        ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
        
        [í…ìŠ¤íŠ¸]: {text}
        
        [ì‘ë‹µ í˜•ì‹]:
        {{
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
            "summary": "í•œ ì¤„ ìš”ì•½",
            "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
            "features": ["ê¸°ëŠ¥1", "ê¸°ëŠ¥2"]
        }}
        """
        try:
            llm_res = await drafter_models["scout"].ainvoke(llm_prompt)
            json_match = re.search(r'\{.*\}', llm_res.content, re.DOTALL)
            if json_match:
                llm_data = json.loads(json_match.group())
            else:
                raise ValueError("No JSON found")
        except Exception as e:
            llm_data = {"keywords": [], "summary": text[:50], "sentiment": "ì¤‘ë¦½", "features": []}

        # ë°ì´í„° ì •ì œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        raw_llm_keywords = llm_data.get("keywords", [])
        clean_llm_keywords = []
        if isinstance(raw_llm_keywords, list):
            for k in raw_llm_keywords:
                if isinstance(k, str): clean_llm_keywords.append(k)
                elif isinstance(k, dict): clean_llm_keywords.extend([str(v) for v in k.values()])
                else: clean_llm_keywords.append(str(k))
        
        raw_features = llm_data.get("features", [])
        clean_features = []
        if isinstance(raw_features, list):
            for f in raw_features:
                if isinstance(f, str): clean_features.append(f)
                elif isinstance(f, dict): clean_features.extend([str(v) for v in f.values()])
                else: clean_features.append(str(f))

        final_keywords = list(set(local_keywords) | set(clean_llm_keywords))
        
        metadata = {
            "keywords": ", ".join([str(k) for k in final_keywords]),
            "summary": str(llm_data.get("summary", text[:100])),
            "sentiment": str(llm_data.get("sentiment", "ì•Œ ìˆ˜ ì—†ìŒ")),
            "features": ", ".join([str(f) for f in clean_features])
        }
        
        return metadata

async def fetch_new_reports_from_db():
    """ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€(an_vectorized_at IS NULL) ë³´ê³ ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor(dictionary=True)
    
    query = """
    SELECT 
        an.an_idx,
        a.a_name as app_name,
        v.v_version as version,
        an.an_text as report_markdown,
        MIN(r.r_date) as latest_review_date
    FROM analytics an
    JOIN version v ON an.v_idx = v.v_idx
    JOIN app a ON v.a_idx = a.a_idx
    JOIN review r ON v.v_idx = r.v_idx
    WHERE an.an_vectorized_at IS NULL
    GROUP BY an.an_idx;
    """
    
    cursor.execute(query)
    rows = cursor.fetchall()
    
    cursor.close()
    conn.close()
    return rows

def update_single_report_timestamp(an_idx):
    """
    [ë³€ê²½] ë‹¨ì¼ ë³´ê³ ì„œì— ëŒ€í•´ ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„(TimeStamp)ì„ DBì— ì—…ë°ì´íŠ¸
    (async í•¨ìˆ˜ ë‚´ì—ì„œ í˜¸ì¶œë˜ì§€ë§Œ, mysql.connectorëŠ” ë™ê¸°ì‹ì´ë¯€ë¡œ ì¼ë°˜ í•¨ìˆ˜ë¡œ ì‘ì„±)
    """
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    try:
        query = "UPDATE analytics SET an_vectorized_at = %s WHERE an_idx = %s"
        cursor.execute(query, (now, an_idx))
        conn.commit()
        # print(f"  â”” [DB] ID {an_idx} ì—…ë°ì´íŠ¸ ì™„ë£Œ") # ë¡œê·¸ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬
    except Exception as e:
        print(f"  â”” âŒ [DB] ID {an_idx} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

async def ingest_db_to_vector():
    # 1. DBì—ì„œ ì²˜ë¦¬ ëŒ€ìƒ ë°ì´í„° ë¡œë“œ
    db_reports = await fetch_new_reports_from_db()
    
    if not db_reports:
        print("ğŸ‰ ëª¨ë“  ë³´ê³ ì„œê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹ ê·œ ë°ì´í„° ì—†ìŒ)")
        return

    print(f"ğŸ“¦ ì‹ ê·œ ë³´ê³ ì„œ {len(db_reports)}ê°œë¥¼ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # 2. Chroma DB ì´ˆê¸°í™” (ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ë¡œë“œ)
    # persist_directoryê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë°ì´í„°ëŠ” íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥ë©ë‹ˆë‹¤.
    vector_store = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )

    extractor = MetadataEnsemble()
    
    # 3. ìŠ¤í”Œë¦¬í„° ì„¤ì •
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[
        ("#", "report_title"), ("##", "category"), ("###", "sub_category")
    ])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)

    # 4. ë³´ê³ ì„œ 1ê°œì”© ì²˜ë¦¬ -> ì €ì¥ -> DBì—…ë°ì´íŠ¸
    for row in tqdm(db_reports, desc="Processing Reports"):
        an_idx = row['an_idx']
        
        try:
            # --- ë‚ ì§œ ë° ê¸°ë³¸ ì •ë³´ ì²˜ë¦¬ ---
            dt = row['latest_review_date']
            if dt is None:
                print(f"âš ï¸ ë‚ ì§œ ì •ë³´ ì—†ìŒ: ID {an_idx} ìŠ¤í‚µ")
                continue
                
            year = str(dt.year)
            month = f"{dt.month:02d}"
            quarter = f"{(dt.month-1)//3 + 1}Q"
            full_date = dt.strftime('%Y-%m-%d')

            # --- í…ìŠ¤íŠ¸ ë¶„í•  ë° ë©”íƒ€ë°ì´í„° ìƒì„± ---
            current_report_chunks = []
            header_splits = md_splitter.split_text(row['report_markdown'])
            
            for doc in header_splits:
                sub_chunks = text_splitter.split_documents([doc])
                for chunk in sub_chunks:
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                    chunk.metadata.update({
                        "source_an_idx": an_idx,
                        "app_name": row['app_name'],
                        "version": row['version'],
                        "year": year,
                        "month": month,
                        "quarter": quarter,
                        "date": full_date
                    })
                    
                    # AI ë©”íƒ€ë°ì´í„° (ë¹„ë™ê¸°)
                    meta_analysis = await extractor.generate_metadata(chunk.page_content)
                    chunk.metadata.update(meta_analysis)
                    
                    current_report_chunks.append(chunk)
            
            # --- [ì¤‘ìš”] 1ê°œ ë³´ê³ ì„œ ì²˜ë¦¬ ëë‚  ë•Œë§ˆë‹¤ ë²¡í„° DBì— ì¦‰ì‹œ ì €ì¥ ---
            if current_report_chunks:
                vector_store.add_documents(current_report_chunks)
                
                # --- [ì¤‘ìš”] MySQL DBì— ì¦‰ì‹œ ì—…ë°ì´íŠ¸ ---
                update_single_report_timestamp(an_idx)
            else:
                print(f"âš ï¸ ID {an_idx}: ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ (ID: {an_idx}): {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ê±´ì€ ë„˜ì–´ê°€ê³  ë‹¤ìŒ ê±´ì„ ì²˜ë¦¬ (DB ì—…ë°ì´íŠ¸ ì•ˆ í•¨)
            continue

    print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(ingest_db_to_vector())