import os
import asyncio
from typing import List
import json
import re

# LangChain Imports
from langchain_groq import ChatGroq
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# ë¡œì»¬ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ (ê°€ì •)
from keybert import KeyBERT
from konlpy.tag import Okt
# from transformers import pipeline # KoBERT ìš”ì•½ìš©

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# ==========================================
# [ì„¤ì •] ëª¨ë¸ ë° ê²½ë¡œ ì •ì˜
# ==========================================
PERSIST_DIRECTORY = "./chroma_advanced_db"

# 1. ì„ë² ë”© (Local Ollama)
# 'ollama pull bge-m3' ì„ í–‰ í•„ìš”
embeddings = OllamaEmbeddings(model="bge-m3") 

# 2. ë¦¬ë­ì»¤ (HuggingFace Local)
reranker_model = HuggingFaceCrossEncoder(model_name="dragonkue/bge-reranker-v2-m3-ko")

# 3. LLM ì„¤ì • (Groq)
# (1) ë©”íƒ€ë°ì´í„° íŒì • ë° ìµœì¢… ì¢…í•©ìš© (ì‹¬íŒ)
judge_llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0, api_key=GROQ_API_KEY)

# (2) ë‹µë³€ ìƒì„±ìš© 3ëŒ€ì¥ (Draft Models)
# ì£¼ì˜: Groqì—ì„œ ì‹¤ì œ ì§€ì›í•˜ëŠ” ëª¨ë¸ IDì—¬ì•¼ í•¨. (ì˜ˆì‹œ ID ì‚¬ìš©)
drafter_models = {
    "scout": ChatGroq(model="llama-3.1-8b-instant", temperature=0.7),      # Llama-4 ëŒ€ì²´ìš© (ì„ì‹œ)
    "qwen": ChatGroq(model="qwen/qwen3-32b", temperature=0.7)                # Qwen3 ëŒ€ì²´ìš© (ì„ì‹œ)
}

# ==========================================
# [Phase 1] ë©”íƒ€ë°ì´í„° ì•™ìƒë¸” ì¶”ì¶œê¸°
# ==========================================
class MetadataEnsemble:
    def __init__(self):
        self.kw_model = KeyBERT()
        self.okt = Okt()
        # self.summ_model = pipeline("summarization", model="kobert-base...") # KoBERT ë¡œë“œ ê°€ì •
    
    def _extract_local_keywords(self, text):
        """KeyBERT + KoNLPy ê²°í•©"""
        nouns = " ".join(self.okt.nouns(text)) # ëª…ì‚¬ë§Œ ì¶”ì¶œí•´ì„œ í›„ë³´êµ° ì••ì¶•
        keywords = self.kw_model.extract_keywords(nouns, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=5)
        return [k[0] for k in keywords]

    async def generate_metadata(self, text):
        """
        ë¡œì»¬ vs LLM ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ Judgeê°€ ë” ë‚˜ì€ ê²ƒì„ ì„ íƒ
        """
        # 1. ë¡œì»¬ ì¶”ì¶œ
        local_keywords = self._extract_local_keywords(text)
        
        # 2. LLM ì¶”ì¶œ (Groq)
        llm_prompt = f"""
        ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
        
        [í…ìŠ¤íŠ¸]: {text}
        
        [ì‘ë‹µ í˜•ì‹]:
        {{
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
            "summary": "í•œ ì¤„ ìš”ì•½",
            "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ ì„ íƒ",
            "features": ["ì–¸ê¸‰ëœ ê¸°ëŠ¥1", "ì–¸ê¸‰ëœ ê¸°ëŠ¥2"]
        }}
        """
        try :
            llm_res = await drafter_models["scout"].ainvoke(llm_prompt)
            json_str = re.search(r'\{.*\}', llm_res.content, re.DOTALL).group()
            llm_data = json.loads(json_str)
        except:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            llm_data = {"keywords": [], "summary": text[:50], "sentiment": "ì¤‘ë¦½", "features": []}

        # 3. ë°ì´í„° ê²°í•© (Ensemble)
        # ë¡œì»¬ì˜ ì •í™•í•œ ë‹¨ì–´ + LLMì˜ ë¬¸ë§¥ ë‹¨ì–´ë¥¼ í•©ì§‘í•©(set)ìœ¼ë¡œ ì²˜ë¦¬
        final_keywords = list(set(local_keywords) | set(llm_data.get("keywords", [])))
        
        # 4. ìµœì¢… ê²°ê³¼ ë°˜í™˜
        metadata = {
            "keywords": ", ".join(final_keywords),
            "summary": llm_data.get("summary", text[:100]),
            "sentiment": llm_data.get("sentiment", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "features": ", ".join(llm_data.get("features", []))
        }
        
        return metadata

# ==========================================
# [Phase 2] ë°ì´í„° ì ì¬ (Ingestion)
# ==========================================
async def ingest_markdown_reports(markdown_reports: list):
    """
    markdown_reports: [{"text": "ë§ˆí¬ë‹¤ìš´ë‚´ìš©", "version": "v1.0.2"}, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    """
    extractor = MetadataEnsemble()
    all_processed_chunks = []

    # 1. ë§ˆí¬ë‹¤ìš´ í—¤ë” ìŠ¤í”Œë¦¬í„° ì„¤ì •
    headers_to_split_on = [
        ("#", "AppTitle"),
        ("##", "Section"),
        ("###", "SubSection"),
    ]
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ 2ì°¨ ìŠ¤í”Œë¦¬í„°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)

    print(f"ğŸš€ ì´ {len(markdown_reports)}ê°œì˜ ë³´ê³ ì„œ ì²˜ë¦¬ ì‹œì‘...")

    for report in markdown_reports:
        # A. êµ¬ì¡°ì  ë¶„í•  (í—¤ë” ê¸°ë°˜)
        header_splits = md_splitter.split_text(report["text"])
        
        # B. ì„¸ë¶€ ë¶„í•  ë° ë©”íƒ€ë°ì´í„° ê°•í™”
        for doc in header_splits:
            # ë„ˆë¬´ ê¸¸ë©´ ìª¼ê°¬ (ê¸°ì¡´ metadataëŠ” ìœ ì§€ë¨)
            sub_chunks = text_splitter.split_documents([doc])
            
            for chunk in sub_chunks:
                # ê¸°ë³¸ ì •ë³´ íƒœê¹…
                chunk.metadata["version"] = report["version"]
                
                # C. [í•µì‹¬] Phase 1ì˜ ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í™œìš©
                # ìª¼ê°œì§„ í…ìŠ¤íŠ¸(chunk.page_content)ì— ëŒ€í•´ í‚¤ì›Œë“œ/ìš”ì•½ ìƒì„±
                metadata_result = await extractor.generate_metadata(chunk.page_content)
                chunk.metadata["keywords"] = metadata_result["keywords"]
                chunk.metadata["summary"] = metadata_result["summary"]
                chunk.metadata["sentiment"] = metadata_result["sentiment"] # ì¶”ê°€ëœ í•„ë“œ í™œìš©
                chunk.metadata["features"] = metadata_result["features"]   # ì¶”ê°€ëœ í•„ë“œ í™œìš©
                all_processed_chunks.append(chunk)

    # 2. ë²¡í„° DB ì €ì¥ (ì„ë² ë”© ìˆ˜í–‰)
    vector_store = Chroma.from_documents(
        documents=all_processed_chunks,
        embedding=embeddings, # bge-m3 ì‚¬ìš©
        persist_directory=PERSIST_DIRECTORY
    )
    
    print(f"ğŸ’¾ {len(all_processed_chunks)}ê°œì˜ ì²­í¬ê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return vector_store

# ==========================================
# [Phase 3] ê²€ìƒ‰ ë° ë¦¬ë­í‚¹
# ==========================================
def get_search_pipeline(vector_store, doc_texts):
    # 1. Base Retrievers
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": 20})
    bm25_retriever = BM25Retriever.from_texts(doc_texts)
    bm25_retriever.k = 20
    
    # 2. Hybrid (Ensemble)
    ensemble = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.4, 0.6]
    )
    
    # 3. Reranking (Cross Encoder)
    compressor = CrossEncoderReranker(model=reranker_model, top_n=5)
    
    final_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=ensemble
    )
    return final_retriever

# ==========================================
# [Phase 4] MoA (Mixture of Agents) ìƒì„±
# ==========================================
async def generate_final_answer(query, context):
    """
    3ê°œì˜ ëª¨ë¸ì´ ì´ˆì•ˆì„ ì‘ì„±í•˜ê³ , Llama-3.3-70Bê°€ ìµœì¢… ì¢…í•©
    """
    print("\nğŸ¤– [MoA] 3ê°œ ëª¨ë¸ì´ ë™ì‹œ ë¶„ì„ ì¤‘...")
    
    prompt_template = """
    [ë¬¸ë§¥]: {context}
    [ì§ˆë¬¸]: {query}
    
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    # 1. 3ê°œ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ (Async)
    tasks = []
    model_names = ["llama-4-scout", "qwen3-32b"]
    
    for name, model in drafter_models.items():
        chain = prompt | model | StrOutputParser()
        tasks.append(chain.ainvoke({"context": context, "query": query}))
        
    results = await asyncio.gather(*tasks)
    
    # 2. ë‹µë³€ ëª¨ìŒ
    candidates = ""
    for name, res in zip(model_names, results):
        candidates += f"\n\n--- [ì˜ê²¬: {name}] ---\n{res}"
        
    print("ğŸ‘¨â€âš–ï¸ [Judge] Llama-3.3-70Bê°€ ìµœì¢… íŒê²° ì¤‘...")
    
    # 3. ìµœì¢… ì¢…í•© (Synthesizer)
    final_prompt = f"""
    ë‹¹ì‹ ì€ ì•± ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì´ì ì¹œì ˆí•œ ìƒë‹´ì›ì…ë‹ˆë‹¤. 
    ì•„ë˜ 3ëª…ì˜ ë¶„ì„ê°€ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    
    [ì§ˆë¬¸]: {query}
    
    [ê²€ìƒ‰ëœ íŒ©íŠ¸ ë°ì´í„°]
    {context}
    
    [ë¶„ì„ê°€ ì´ˆì•ˆ ëª¨ìŒ]
    {candidates}
    
    [ë‹µë³€ ê°€ì´ë“œë¼ì¸]
    1. ë§íˆ¬ëŠ” "~í•´ìš”", "~ì…ë‹ˆë‹¤"ì™€ ê°™ì€ ì¹œì ˆí•œ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    2. ë³´ê³ ì„œ í˜•ì‹ì´ ì•„ë‹Œ, ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ë§Œ ì§šì–´ì£¼ëŠ” 'ì±„íŒ… ë‹µë³€' í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
    3. íŒ©íŠ¸ ë°ì´í„°ì— ê·¼ê±°í•˜ë˜, ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ì„œë¡ ì´ë‚˜ ê²°ë¡ ì€ ìƒëµí•˜ê³  ë°”ë¡œ ë³¸ë¡ ì„ ë§ì”€í•˜ì„¸ìš”.
    4. ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ ë²„ì „ ì •ë³´ê°€ ìˆë‹¤ë©´ ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
    """
    
    final_chain = judge_llm | StrOutputParser()
    final_answer = await final_chain.ainvoke(final_prompt)
    
    return final_answer

# ==========================================
# [ë©”ì¸ ì‹¤í–‰ ë¡œì§]
# ==========================================
async def main():
    # ì‹¤í—˜ìš© ê°€ìƒ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ë°ì´í„°
    markdown_reports = [
        {
            "version": "v1.0.0",
            "text": """# ğŸ“± [ë„·í”Œë¦­ìŠ¤] ë²„ì „ë³„ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ
## 1. ğŸ“‘ ë³´ê³ ì„œ ê°œìš”
| í•­ëª© | ë‚´ìš© |
| :--- | :--- |
| **ë¶„ì„ ëŒ€ìƒ ë²„ì „** | v1.0.0 |
| **ì‚¬ìš©ì í‰ì ** | 3.5 / 5.0 |

## 2. ğŸ“Š ì¢…í•© ìš”ì•½
### 2.1 ì´í‰
ì´ˆê¸° ëŸ°ì¹­ ë²„ì „ìœ¼ë¡œ ì½˜í…ì¸  ì–‘ì€ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë‚˜, ì•± ì‹¤í–‰ ì†ë„ê°€ ëŠë¦¬ê³  UI ë‚´ë¹„ê²Œì´ì…˜ì´ ë³µì¡í•˜ë‹¤ëŠ” ì˜ê²¬ì´ ë§ìŒ.

## 3. ğŸš¨ ìƒì„¸ ì´ìŠˆ ë¶„ì„
### 3.1 ì¬ìƒ ëŠê¹€ (ì–¸ê¸‰ëŸ‰: ìƒ, ë¶€ì • ë¹„ìœ¨: 45%)
**ğŸ’¬ ëŒ€í‘œ VOC**
> ğŸ’¥ **Problem**: "ì˜ìƒì„ ë³´ë‹¤ê°€ ìê¾¸ ë²„í¼ë§ì´ ê±¸ë ¤ì„œ ëª°ì…ë„ê°€ ë–¨ì–´ì ¸ìš”."
**ğŸ”§ ê°œì„  ê°€ì´ë“œë¼ì¸**
- ì„œë²„ ìºì‹± ë¡œì§ ìµœì í™” í•„ìš”.
"""
        },
        {
            "version": "v2.0.0",
            "text": """# ğŸ“± [ë„·í”Œë¦­ìŠ¤] ë²„ì „ë³„ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ
## 1. ğŸ“‘ ë³´ê³ ì„œ ê°œìš”
| í•­ëª© | ë‚´ìš© |
| :--- | :--- |
| **ë¶„ì„ ëŒ€ìƒ ë²„ì „** | v2.0.0 |
| **ì‚¬ìš©ì í‰ì ** | 4.5 / 5.0 |

## 2. ğŸ“Š ì¢…í•© ìš”ì•½
### 2.1 ì´í‰
UI ê°œí¸ì„ í†µí•´ ì‚¬ìš©ì„±ì´ í¬ê²Œ ê°œì„ ë¨. íŠ¹íˆ v1.0.0ì—ì„œ ì§€ì ëœ ë²„í¼ë§ ë¬¸ì œê°€ ê±°ì˜ í•´ê²°ë˜ì–´ ê¸ì •ì  ë°˜ì‘ì´ ì§€ë°°ì ì„.

## 3. ğŸš¨ ìƒì„¸ ì´ìŠˆ ë¶„ì„
### 3.1 ìë§‰ ê°€ë…ì„± (ì–¸ê¸‰ëŸ‰: ì¤‘, ë¶€ì • ë¹„ìœ¨: 20%)
**ğŸ’¬ ëŒ€í‘œ VOC**
> ğŸ’¥ **Problem**: "ìë§‰ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì•„ì„œ íƒœë¸”ë¦¿ìœ¼ë¡œ ë³¼ ë•Œ ë¶ˆí¸í•´ìš”."
**ğŸ”§ ê°œì„  ê°€ì´ë“œë¼ì¸**
- ìë§‰ í¬ê¸° ë° ë°°ê²½ìƒ‰ ì»¤ìŠ¤í…€ ì„¤ì • ê¸°ëŠ¥ ì¶”ê°€ ê²€í† .
"""
        }
    ]
    
    # 1. ë¬¸ì„œ ì ì¬
    vector_store = await ingest_markdown_reports(markdown_reports)
    
    # 2. ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    all_docs = vector_store.get()["documents"]
    retriever = get_search_pipeline(vector_store, all_docs)
    
    # 3. ì‚¬ìš©ì ì§ˆë¬¸: ë²„ì „ë³„ ë¹„êµ ì§ˆë¬¸
    query = "ë„·í”Œë¦­ìŠ¤ v1.0.0ê³¼ v2.0.0ì„ ë¹„êµí–ˆì„ ë•Œ, ì‚¬ìš©ì í‰ì ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì¡Œë‚˜ìš”?"
    
    # 4. ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    retrieved_docs = retriever.invoke(query)
    context = "\n".join([f"- {d.page_content} (ì¶œì²˜: {d.metadata.get('version')})" for d in retrieved_docs])
    
    final_answer = await generate_final_answer(query, context)
    
    print("\n" + "="*50)
    print("ğŸ“ [ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸]")
    print("="*50)
    print(final_answer)

if __name__ == "__main__":
    asyncio.run(main())