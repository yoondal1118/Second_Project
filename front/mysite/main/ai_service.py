import os
import asyncio
from django.conf import settings
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_groq import ChatGroq
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

PERSIST_DIRECTORY = "./RAG/chroma_advanced_db"

print(f"ğŸ“‚ [System] AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘... (DB ê²½ë¡œ: {PERSIST_DIRECTORY})")

# ì „ì—­ ë³€ìˆ˜
retriever = None 
llm = None  # ë‹¨ì¼ ëª¨ë¸ ë³€ìˆ˜

try:
    embeddings = OllamaEmbeddings(model="bge-m3")
    reranker_model = HuggingFaceCrossEncoder(model_name="dragonkue/bge-reranker-v2-m3-ko")

    # [ìˆ˜ì • 1] ë³µì¡í•œ ëª¨ë¸ dict ì œê±°í•˜ê³  ë©”ì¸ ëª¨ë¸ í•˜ë‚˜ë§Œ ì •ì˜
    # Llama-3.3-70bëŠ” ì»¨í…ìŠ¤íŠ¸ ì´í•´ë ¥ì´ ì¢‹ì•„ ë°”ë¡œ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0, streaming=True) 

    # ë²¡í„° DB ë¡œë“œ
    vector_store = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings
    )

    # ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€ - ê²€ìƒ‰ í’ˆì§ˆì€ ì¤‘ìš”í•˜ë¯€ë¡œ)
    def get_search_pipeline(vector_store):
        try:
            all_docs_data = vector_store.get()
            texts = all_docs_data["documents"]
            metadatas = all_docs_data["metadatas"]
            
            if not texts:
                print("âš ï¸ [Warning] ë²¡í„° DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            doc_objects = [Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas)]

            vector_retriever = vector_store.as_retriever(search_kwargs={"k": 15})
            bm25_retriever = BM25Retriever.from_documents(doc_objects)
            bm25_retriever.k = 15
            
            ensemble = EnsembleRetriever(
                retrievers=[bm25_retriever, vector_retriever],
                weights=[0.4, 0.6]
            )
            
            compressor = CrossEncoderReranker(model=reranker_model, top_n=5)
            final_retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=ensemble
            )
            return final_retriever
        except Exception as e:
            print(f"âŒ Pipeline Init Error: {e}")
            return None

    retriever = get_search_pipeline(vector_store)
    
    if retriever:
        print("âœ… [System] AI ëª¨ë¸ ë° DB ë¡œë“œ ì™„ë£Œ")
    else:
        print("âš ï¸ [System] ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨")

except Exception as e:
    print(f"âŒ [Critical] AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    retriever = None


async def generate_chat_response(query, valid_apps):
    if not retriever or not llm:
        yield "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    # ëŒ€ì†Œë¬¸ìë‚˜ ê³µë°± ì°¨ì´ë¡œ ì¸í•œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì „ì²˜ë¦¬
    valid_apps_clean = [app.strip().replace(" ", "").lower() for app in valid_apps]
    valid_apps_str = ", ".join([f"'{app}'" for app in valid_apps]) or "(ë“±ë¡ëœ ì•± ì—†ìŒ)"

    # 1. ê²€ìƒ‰ ìˆ˜í–‰
    try:
        retrieved_docs = await asyncio.to_thread(retriever.invoke, query)
        print(f"ğŸ” [Debug] ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}ê°œ") # ë””ë²„ê¹…ìš©
    except Exception as e:
        print(f"âŒ Retrieval Error: {e}")
        yield "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return

    # 2. ë¬¸ë§¥ êµ¬ì„± ë° í•„í„°ë§
    context_list = []
    used_docs = []

    if retrieved_docs:
        for d in retrieved_docs:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì•± ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'ì•Œìˆ˜ì—†ìŒ')
            raw_app_name = d.metadata.get('app_name', 'ì•Œìˆ˜ì—†ìŒ')
            clean_app_name = raw_app_name.strip().replace(" ", "").lower()
            
            # [ìˆ˜ì • í¬ì¸íŠ¸] ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ í—ˆìš© (ë¶€ë¶„ ì¼ì¹˜ ë¡œì§)
            is_valid = any(app in clean_app_name or clean_app_name in app for app in valid_apps_clean)
            
            if is_valid:
                version_info = d.metadata.get('version', 'ì•Œìˆ˜ì—†ìŒ')
                source_info = f"[[ë¶„ì„ ëŒ€ìƒ ì•±: {raw_app_name}, ë²„ì „: {version_info}]]"
                summary_text = d.metadata.get('summary', '')
                full_text = f"{source_info}\nìš”ì•½: {summary_text}\nìƒì„¸ë‚´ìš©: {d.page_content}"
                
                context_list.append(full_text)
                used_docs.append(d)
            else:
                print(f"âš ï¸ [Filtered] í—ˆìš©ë˜ì§€ ì•Šì€ ì•± ì œì™¸ë¨: {raw_app_name}")

    # ë¬¸ë§¥ì´ ì—†ëŠ” ê²½ìš° (ê²€ìƒ‰ì€ ëìœ¼ë‚˜ í•„í„°ì—ì„œ ë‹¤ ê±¸ëŸ¬ì§„ ê²½ìš° í¬í•¨)
    if not context_list:
        yield (
            f"ê²€ìƒ‰ ê²°ê³¼, í˜„ì¬ ë“±ë¡ëœ ì•±({valid_apps_str}) ëª©ë¡ì—ì„œ **'{query}'**ì™€ ê´€ë ¨ëœ ìƒì„¸ ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
            "ì•± ì´ë¦„ì´ ì •í™•í•œì§€, í˜¹ì€ í•´ë‹¹ ì•±ì— ëŒ€í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
        return

    # --- ì´í›„ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ì€ ë™ì¼í•˜ê²Œ ìœ ì§€ ---
    context = "\n\n".join(context_list)

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë™ì¼í•¨)
    rag_prompt_template = f"""
    ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ì•± ë¦¬ë·° ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ê²€ìƒ‰ëœ ë³´ê³ ì„œ ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

    [ì‚¬ìš©ìì˜ ë“±ë¡ëœ ì•± ëª©ë¡]
    {valid_apps_str}

    [ê²€ìƒ‰ëœ ë³´ê³ ì„œ ë¬¸ë§¥]
    {{context}}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {{query}}

    [ğŸš¨ ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸]
    1. **ì•± ì´ë¦„ ë° ë²„ì „ ìœ ì—°ì„±**: 
       - ì§ˆë¬¸ì— í¬í•¨ëœ ì•± ì´ë¦„ì´ [ë“±ë¡ëœ ì•± ëª©ë¡]ì— í¬í•¨ëœë‹¤ë©´ ì •ìƒì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
       - ì§ˆë¬¸í•œ íŠ¹ì • 'ë¹Œë“œ ë²ˆí˜¸'ê°€ ë¬¸ë§¥ì— ì—†ë”ë¼ë„, ë¬¸ë§¥ìƒ ê°€ì¥ ìœ ì‚¬í•˜ê±°ë‚˜ ìµœì‹  ë²„ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , "ì •í™•í•œ ë¹Œë“œ ë²ˆí˜¸ëŠ” ì—†ì§€ë§Œ ìœ ì‚¬ ë²„ì „ì— ë”°ë¥´ë©´..."ì´ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”.
    
    2. **ë°ì´í„° ê¸°ë°˜ ë‹µë³€**:
       - ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "í•´ë‹¹ ë‚´ìš©ì€ ë³´ê³ ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.
       - "NO_DATA", "NOT_REGISTERED" ê°™ì€ ì‹œìŠ¤í…œ ì½”ë“œë¥¼ ì¶œë ¥í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

    3. **ì¼ìƒ ëŒ€í™” ì²˜ë¦¬**:
       - "ì•ˆë…•", "ê³ ë§ˆì›Œ" ê°™ì€ ì¸ì‚¬ë§ì—ëŠ” ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ì— ë§ì¶° ì •ì¤‘í•˜ê²Œ ì¸ì‚¬í•˜ì„¸ìš”.

    4. **ìŠ¤íƒ€ì¼**:
       - ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì–´ì²´(~í•´ìš”)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    
    prompt = ChatPromptTemplate.from_template(rag_prompt_template)
    
    # ì²´ì¸ ìƒì„±
    chain = prompt | llm | StrOutputParser()
    
    # 4. [í•µì‹¬ ìˆ˜ì •] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (astream ì‚¬ìš©)
    try:
        # ainvoke ëŒ€ì‹  astreamì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # chunkëŠ” LLMì´ ë±‰ì–´ë‚´ëŠ” í•œ ê¸€ì(ë˜ëŠ” í† í°) ë‹¨ìœ„ì…ë‹ˆë‹¤.
        async for chunk in chain.astream({"context": context, "query": query}):
            yield chunk  # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ê°ì„ ë˜ì ¸ì¤Œ
            
    except Exception as e:
        print(f"Generation Error: {e}")
        yield "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return

    # 5. ì¶œì²˜ í‘œì‹œ (ë‹µë³€ì´ ë‹¤ ëë‚œ ë’¤ ë§ˆì§€ë§‰ì— ë¶™ì„)
    if used_docs:
        unique_sources = set()
        priority_docs = [d for d in used_docs if str(d.metadata.get('version', '')) in query]
        final_docs_to_show = priority_docs if priority_docs else used_docs

        for doc in final_docs_to_show:
            title = doc.metadata.get('report_title', 'ë¶„ì„ ë³´ê³ ì„œ')
            date = doc.metadata.get('date', '')
            version = doc.metadata.get('version', '')
            
            source_parts = [f"- {title}"]
            if version: source_parts.append(f"**[v{version}]**")
            if date: source_parts.append(f"({date})")
            
            unique_sources.add(" ".join(source_parts))

        if unique_sources:
            sources_text = "\n".join(sorted(list(unique_sources)))
            # ë§ˆì§€ë§‰ì— ì¶œì²˜ ì •ë³´ë¥¼ yield
            yield f"\n\n---\n**ğŸ“š ì°¸ê³  ë¬¸ì„œ**\n{sources_text}"